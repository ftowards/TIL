Linear Regression 선형회귀

  통계학에서, 선형 회귀(線型回歸, 영어: linear regression)는 종속 변수 y와 한 개 이상의 독립 변수 (또는 설명 변수) X와의 선형 상관 관계를 모델링하는 회귀분석 기법이다.
  한 개의 설명 변수에 기반한 경우에는 단순 선형 회귀(simple linear regression), 둘 이상의 설명 변수에 기반한 경우에는 다중 선형 회귀라고 한다.
  
  선형 회귀는 선형 예측 함수를 사용해 회귀식을 모델링하며, 알려지지 않은 파라미터는 데이터로부터 추정한다. 이렇게 만들어진 회귀식을 선형 모델이라고 한다.  
  선형 회귀는 깊이있게 연구되고 널리 사용된 첫 번째 회귀분석 기법이다.
  이는 알려지지 않은 파라미터에 대해 선형 관계를 갖는 모델을 세우는 것이, 비선형 관계를 갖는 모델을 세우는 것보다 용이하기 때문이다.

  선형 회귀는 여러 사용 사례가 있지만, 대개 아래와 같은 두 가지 분류 중 하나로 요약할 수 있다.

  값을 예측하는 것이 목적일 경우, 선형 회귀를 사용해 데이터에 적합한 예측 모형을 개발한다. 개발한 선형 회귀식을 사용해 y가 없는 x값에 대해 y를 예측하기 위해 사용할 수 있다.
  종속 변수 y와 이것과 연관된 독립 변수 X1, ..., Xp가 존재하는 경우에, 선형 회귀 분석을 사용해 Xj와 y의 관계를 정량화할 수 있다. 
  Xj는 y와 전혀 관계가 없을 수도 있고, 추가적인 정보를 제공하는 변수일 수도 있다.
  
  일반적으로 최소제곱법(least square method)을 사용해 선형 회귀 모델을 세운다. 최소제곱법 외에 다른 기법으로도 선형 회귀 모델을 세울 수 있다. 
  손실 함수(loss fuction)를 최소화 하는 방식으로 선형 회귀 모델을 세울 수도 있다. 최소제곱법은 선형 회귀 모델 뿐 아니라, 비선형 회귀 모델에도 적용할 수 있다. 
  최소제곱법과 선형 회귀는 가깝게 연관되어 있지만, 그렇다고 해서 동의어는 아니다.
  
  출처 : 위키 https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80
  
  - row = n 개의 feature(column) 을 가진 feature vector 가 됨
  
Gradient Boosting
  
  - 참고) 머신러닝 - 15. 그레디언트 부스트(Gradient Boost) 
    https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-15-Gradient-Boost
    
------------------------------------------------------------
OLS : Ordinary Least Squares
  최소자승법, 최소이승법, 최소제곱법
  
  1. 개요
     주로 통계에 대해 처음 배울 때 접하는 모형으로 매우 단순하지만 많은 곳에서 쓰인다. 이를 가장 단순히 써보면 다음과 같다.
     Y=aX+b
     어떤 종속변수 Y가 어떤 독립변수(들) X들의 선형 결합을 통해 결정된다고 보는 것이다.
     
     가우스-마르코브 조건(Gauss-Markov assumption)을 만족시킬 때, BLUE(best linear unbiased estimator, 최량선형 불편추정량) 임을 증명할 수 있다.
     BLUE 는 불편성 기댓값이 모수와 일치(즉 모형을 통해 추정한 값들의 평균이 참값임)을 갖추고 가장 효율적인(통계학이나 계량경제학에서 효율성은 분산이 작다는 뜻) 추정방법이다.
     
  2. 가우스-마르코브 조건
    2.0.1 선형성
        실제 추정하고자 하는 현실이 선형적인 모델로 설명될 수 있어야 할 것.
        >> 독립 변수들에 대해 편미분하면 상수가 나와야 함.
        이 조건을 만족하지 못 하면 최소자승법으로 추정한 모델은 기본적으로 잘못된, 의미없는 추정이 된다.
    2.0.2 오차항의 평균은 0 : exogeniety
    2.0.3 동분산성 (homoskedasticity)
        어느 시점에서 관측하더라도 동일한 분산이 나올 것
        이를 만족하지 못하는 경우를 가리켜 이분산(heteroskedasticity)이 존재한다고 하며, 이 경우 추정된 값들의 유의성을 담보할 수 없다.
    2.0.4 오차항은 서로 독립
        어떤 시점의 오차항과 다른 시점의 오차항 사이의 공분산이 0 일 것
        이를 만족하지 못할 경우 공간상관성이나(패널/횡단면) 자기상관성(시계열)이 존재하며, OLS 는 더이상 가장 좋은 추정방법이 되지 못 한다.
    2.0.5 독립변수는 주어진 것으로 가정
        1. 주어진 것이라는 표현은 비확률 변수라는 것이지 상수라는 뜻은 아니다.
          비확률변수는 기댓값, 분산 등 적률의 계싼에서 상수처럼 다룰 수는 있지만 엄연히 변수이지 상수는 아니다.
        2. 사회 자료는 비확률변수라는 조건도 너무 강하기 때문에, 
          '독립변수는 확률변수이다. 그리고 오차항에 대해 독립이다' 라는 완화된 조건을 사용하기도 한다.
        3. 사회 자료는 위의 조건마저도 강하게 어기는 경우가 많기 때문에 더 완화된 조건인 '조건부 0' 가정을 사용하는 경우가 더 일반적이다,.
          즉 독립변수 행렬 X 와 오차항 벡터 e 데 대해 'E(e|X) = 0' 가정을 사용한다.
        4. 때로는 위의 가정도 깨져서 각각의 독립변수와 오차항의 상관계수가 0 이라는 조건을 사용해야 하는 경우도 있다.
          즉 cov(e,x) = 0 을 사용한다.
          
    2.0.1 ~ 2.0.4 번 조건을 만족하고 2.0.5의 3번 조건까지만 만족한다면 OLS 추정량은 BLUE 이다.
    그러나 5.3 이 만족되지 않고 5.4 가 만족되면 이 때부터는 BLUE 가 아니다. 이 때부터는 추정량에 bias 가 존재한다.
    다만 5.4 가 만족된다면 OLS 추정량은 일치 추정량(consistent estimator) 이다.
    즉 편의가 있지만 대표본 하에서는 추정량이 참값으로 확률 수렴한다.
    
  3. GLS
    
    오차항에 이분산성이나 자기상관성이 있는 경우에 대해서는 OLS(통상 최소제곱)가 아니라 GLS(Generalized Least Squares, 일반화 최소제곱)를 사용할 수 있다.
    오차항의 이분산 구조나 자기상관구조를 활용한다면 이를 상쇄할 수 있는 함수를 사용하는 최소제곱방법이다.
    OLS 에서는 오차항의 크기만큼 가중치가 주어지는데, GLS 에서는 오차항의 이분산성이나 자기상관성을 상쇄할 수 있도록 보정된 가중치를 부여하여
    최소제곱 추정을 한다고 생각하면 편하다.
    
    그런데 선형확률모형 같은 특별한 케이스가 아닌 이상에야 오차항의 함수 구조를 안다고 하는 조건 자체가 비현실적이다.
    따라서 요즘에는 GLS를 잘 사용하지 않고, 이분산성이나 자기상관성에 대해서도 강건한(robust) 표준오차 추정량을 사용하는 경우가 일반적이다.
    다만 잔차의 그래프나 플롯을 그렸을 때 주어진 자료의 이분상성이나 자기상관성이 너무 강할 때에는 GLS 와 robust 추정을 함께 쓰기도 한다.
    
---------------------------------------------------------------------------- 

MSE(Mean Squared Error) : 평균제곱오차

  선형 회귀 모델을 평가하는 방법
  
  실제 데이터와 예측 값 간의 오차의 크기를 더 쉽게 구하기 위해 오차에 제곱을 하고 그 평균을 구한 값을 말한다.
  이 값을 가장 작게 만들어주는 모델을 찾는 것이 선형 회귀 모델의 목적이다.
  
  
  
