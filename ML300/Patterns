### **학습목표**

- 각 모델들의 개괄적 이해 및 구축과정 학습.
- 파라미터를 최적화 시키는 GridSearch에 대한 이해 및 코딩작업 체화.
- 각 모델의 중요 파라미터에 대한 개괄적 이해.
- 파라미터의 변화에 따른 예측력 변화 경향성 파악.
- 최적의 모형 및 파라미터를 찾는 과정에 대한 계획 수립 및 수행.

* 원 핫 인코딩 상태 >> 범주화

 T / F 의 시리즈 리스트로 변환
 > np.select( 변환한 시리즈 리스트, 범주화할 타겟 리스트(원핫 인코딩의 컬럼들))


1. Logistic Regression

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
from sklearn import metrics

# (문제) 로지스틱 회귀분석 모형을 만들어 lm에 저장합니다. solver는 'liblinear'로 설정합니다.

lm = LogisticRegression(solver = 'liblinear')

    solver{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’
    Algorithm to use in the optimization problem.

    For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones.

    For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss; ‘liblinear’ is limited to one-versus-rest schemes.

    ‘newton-cg’, ‘lbfgs’, ‘sag’ and ‘saga’ handle L2 or no penalty
    ‘liblinear’ and ‘saga’ also handle L1 penalty
    ‘saga’ also supports ‘elasticnet’ penalty
    ‘liblinear’ does not support setting penalty='none'

    출처 : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html

### 문제 10. [로지스틱 회귀분석] Grid Search 구축 (Lidge, Lasso Penalty / Threshold)

# (문제) 로지스틱에서 고려해야할 Penalty의 형태 (Ridge, Lasso), regularization parameter range를 설정하여 이를 parameters에 dictionary 형태로 저장합니다.
parameters = {'penalty' :['l1','l2'],
              'C': [0.01,0.1,0.5,0.9,1,5,10], 'tol' : [1e-4, 1e-6, 1, 1e2]}
    
    모델의 과적합을 방지하기 위해 모델을 정규화(일반화 시키는 방법)
    
    Lasso L1 Regulazation
      변수의 갯수를 조절하여 축소할 수 있다.
      중요성이 떨어지는 변수 먼저 조정하고, 변수 간 상관관계가 높으면 성능이 떨어진다.
      
    Ridge L2 Regulazation
      변수의 중요성을 조절하여 일반화
      중요성이 높은 변수를 먼저 조정하고, 변수 간 상관성이 높아서 모델의 성능이 떨어지지 않는다.
    

# (문제) sklearn.model_selection.GridSearchCV를 활용해 cv값 10, n_jobs값은 n_thread로, scoreing은 "accuracy"로 Grid Search를 세팅하고 이를 GSLR에 저장합니다.
GSLR = GridSearchCV(lm, parameters, cv=10, n_jobs=n_thread, scoring ='accuracy')

    출처 : https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV

# (문제) Grid Search를 fit함수를 활용하여 수행합니다.
GSLR.fit(X_train, y_train)

# 최적의 파라미터 값 및 정확도 (Accuracy) 출력
print('final params', GSLR.best_params_)   
print('best score', GSLR.best_score_)  

### 문제 11. [로지스틱 회귀분석] 모형 평가 및 최적 로지스틱 모형 구축

# (문제) predict 함수를 활용하여 예측 값을 구해 이를 predicted 에 저장합니다.
predicted = GSLR.predict(X_test)

# (문제) sklearn.metrics.confusion_matrix 활용하여 confusion_matrix를 구하고 이를 출력합니다.

CMatrix = confusion_matrix(y_test, predicted)
print(CMatrix)
print("Accuacy : ", GSLR.score(X_test, y_test))

# (문제) sklearn.metrics.classification_report를 활용하여 report를 출력합니다.
print(metrics.classification_report(y_test, predicted))

# Cross validation 과정에서 계산된 정확도 값들을 출력해줍니다.
means = GSLR.cv_results_['mean_test_score']
stds = GSLR.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, GSLR.cv_results_['params']):
    print("%0.3f (+/-%0.03f) for %r"
          % (mean, std * 2, params))
print()



2. 의사 결정 나무

 의사결정나무에서 고려해야할 criterion, min_samples_split, max_depth, min_samples_leaf, max_features
  criterion : measure 선택
  min_sample_split / max_dapth / min_samples_leaf / max_features : 모델 복잡도에 연관
  모델 복잡성을 낮추는 과정 >> Prooning
  
  
 3. Random Forest
 
  - Random Forest는 아래의 Bagging과 Drop-out을 활용하여 의사결정나무의 변동성을 완화시키고 예측력을 높인 모델이다.
  - Bootstrapping: 복원추출을 통하여 샘플 구성이 조금씩 다른 여러 데이터셋을 생성해냄.
  - Aggregating: 여러 모형의 결과를 통합하여 모형의 변동성을 낮춤.
  - Drop-out: Tree를 구성할 때 변수를 일부 탈락시킴. Tree간의 correlation을 감소시켜 이 또한 모형의 변동성을 낮춤.
  
  decision tree 를 결합하여 예측력을 높혔지만, 해석력은 떨어짐
  여러 모델을 결합하였기 때문에 명확하게 결과에 대한 해석을 내놓기 어려울 수 있음
  
 4. SVM
  
  서포트 벡터 머신
  
 
 5. 인공신경망
 
   신경망 모형은 위와 같이 입력 데이터를 종합하여 결과값을 내는 구조를 가진 Perceptron을 중첩시키고 혼합시킨 구조이다. 
   아래와 같이 두 부분으로 나누어볼 수 있다.

   입력값들의 선형합 구조인 transfer function
   activation function f()

   이 때 입력값은 다른 perceptron의 출력값이 될 수 있으며 이것이 중첩되면 아래와 같이 나타날 수 있으며 이를 신경망 모형이라 한다.
    Input Layer: 입력 데이터가 위치하는 layer.
    Hidden Layer: 입력 데이터 혹은 또 다른 hidden layer의 출력값을 입력값으로 하는 perceptron이 위치하는 layer.
    Output Layer:마지막 hidden layer의 출력값을 입력값고 출력함수의 결과를 얻은 노드로 구성된 layer.
  
 
 6. Boosting
 
   학습 데이터에서 여러 번 분류
   오분류 된 데이터에 가중치를 더 주고 반복함으로써 보다 적합한 결과들을 도출하고
   그 결과들을 결합하여 모델을 형성
   
   
 
 
 
